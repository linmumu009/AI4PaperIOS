{
  "institution": "Mila",
  "short_title": "用约束GRPO实现稳定行为控制",
  "📖标题": "Constrained Group Relative Policy Optimization",
  "🌐来源": "arxiv",
  "paper_id": "2602.05863",
  "🛎️文章简介": {
    "other": [
      "如何在无需 critic 的 GRPO 框架中可靠施加显式行为约束，避免 Lagrangian 优化因优势估计失真而失效？",
      "提出 Constrained GRPO，首次将 Lagrangian 约束优化融入 critic-free 的 GRPO，并证明标量化优势是保障乘子语义一致性的关键设计。"
    ]
  },
  "📝重点思路": [
    "🔸构建基于指示成本函数与可学习 Lagrange 乘子的约束优化框架，直接优化违反率；",
    "🔸揭示 GRPO 组内标准化对 scalarized rewards 的隐式重加权机制——其有效权重由各分量方差与协方差决定，破坏乘子设定的权衡意图；",
    "🔸严格推导定理 4.1 量化该失真，并提出“先独立组标准化、再线性组合”的 scalarized advantages 构造法；",
    "🔸在 toy gridworld 中复现约束失效现象（如违反率归零、乘子失焦），并在 NAVSIM-v2 自动驾驶仿真中以软约束优化 NC/DAC/DDC，提升 EPDMS 与任务成功率。"
  ],
  "🔎分析总结": [
    "🔸scalarized rewards 导致权重剧烈波动且与乘子不匹配，约束率被过度压制（如 lava 违反率趋近 0），无法利用容许预算；",
    "🔸scalarized advantages 实现 multiplier 与约束率动态耦合：DAC 满足后 NC multiplier 快速上升，体现自适应资源分配；",
    "🔸固定权重下，scalarized advantages 始终达成更高目标率、更平滑稳定的约束调节，误差条更窄；",
    "🔸硬约束乘积奖励（如 EPDMS）引发稀疏梯度与训练崩溃，而软约束+scalarized advantages 提供持续梯度，保障稳定收敛与泛化。"
  ],
  "💡个人观点": "该工作揭示并修复了GRPO与Lagrangian方法结合时reward-level与advantage-level标准化的理论错配，证明优势标量化是保持乘子语义的必要条件；通过定理4.1、数值例证及gridworld/NAVSIM实验验证其有效性。"
}