{
  "institution": "Microsoft",
  "short_title": "用自监督学习构建世界模型",
  "📖标题": "Reinforcement World Model Learning for LLM-based Agents",
  "🌐来源": "arxiv",
  "paper_id": "2602.05842",
  "🛎️文章简介": {
    "🔸研究问题": "如何在不依赖专家数据、强语言模型或任务成功奖励的情况下，提升LLM代理对环境动态的建模能力？",
    "🔸主要贡献": "提出Reinforcement World Model Learning（RWML），一种基于sim-to-real嵌入空间对齐的自监督强化学习方法，使LLM能稳健学习动作条件下的世界模型。"
  },
  "📝重点思路": [
    "🔸构建动作-状态-下一状态三元组⟨s≤t, aₜ, sₜ₊₁⟩，通过代理自主交互采集无标注训练数据。",
    "🔸设计基于Qwen3-Embedding的sim-to-real奖励r^WM，以预测状态ŝₜ₊₁与真实sₜ₊₁的余弦相似度衡量语义一致性。",
    "🔸采用GRPO优化该奖励，强制模型在输出sₜ₊₁前生成显式推理块（<think>…</think>）。",
    "🔸引入易样本剔除：用轻量SFT模型初筛高奖励样本，并以p=0.1概率保留，聚焦中高难度建模任务。"
  ],
  "🔎分析总结": [
    "🔸RWML在ALFWorld和τ²Bench上分别超越基线19.6和7.9分，显著优于过拟合token的WM SFT。",
    "🔸RWML+Policy RL优于纯任务奖励RL（+6.9/+5.7分），性能媲美专家数据训练，验证其中训练有效性。",
    "🔸RWML比WM SFT引发更少参数更新（尤其MLP与注意力），通用能力退化更小（MMLU、GSM8k）。",
    "🔸RWML显著降低无效动作比例（ALFWorld↓19.85%，τ²Bench错误tool call↓16.06%），提升决策质量。"
  ],
  "💡个人观点": "该工作将世界建模从监督式token预测转向强化式语义对齐：强调环境动态的语义保真而非字面还原；以冻结嵌入空间替代LLM-as-a-judge，提供稳定、可微、抗噪声的信号；通过“自采集-自筛选-自优化”闭环实现端到端自监督。"
}