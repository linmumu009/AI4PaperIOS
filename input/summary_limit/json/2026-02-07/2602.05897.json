{
  "institution": "哈工大",
  "short_title": "提升推理步骤忠实性的强化学习方法",
  "📖标题": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models",
  "🌐来源": "arxiv",
  "paper_id": "2602.05897",
  "🛎️文章简介": {
    "🔸研究问题": "如何在小推理模型中有效抑制链式推理过程中中间步骤的忠实性幻觉，避免仅依赖最终答案正确性带来的错误强化？",
    "🔸主要贡献": "提出FaithRL框架，首次将显式步骤级忠实性奖励与隐式截断重采样机制结合，实现对幻觉推理步骤的精准惩罚与忠实前缀的正向强化。"
  },
  "📝重点思路": [
    "🔸设计显式步骤级奖励：用PRM逐句判别CoT步骤是否忠实上下文（±1），叠加信息增益奖励（控句数/长度）与n-gram重复惩罚。",
    "🔸构建隐式步骤级奖励：提出动态截断重采样（DTR），幻觉处截断并以忠实前缀重生成，形成token级“幻觉受罚、忠实受益”对比信号。",
    "🔸分层奖励分配：答案错误则负奖励均摊至整条CoT；答案正确则仅对各忠实步骤分配细粒度正奖励。",
    "🔸联合优化目标：在GRPO框架下同步优化答案准确率与CoT忠实率，突破终局奖励的强化学习范式。"
  ],
  "🔎分析总结": [
    "🔸SRMs的CoT幻觉率显著高于答案幻觉率（如DPSK-1.5B：CoT忠实率6% vs 答案67.4%），表明仅优化答案会掩盖推理缺陷。",
    "🔸幻觉CoT生成的新问题导致基线模型错误率达59.48%，证实其具有强泛化危害性，不宜作正样本训练。",
    "🔸移除信息增益奖励使准确率下降42.4%，验证其对抑制冗长幻觉的关键作用；DTR机制在HotpotQA等复杂任务中显著提升长程推理稳定性。",
    "🔸FaithRL在多个SRM及5个开放问答基准上平均提升忠实率3.48%、准确率3.86%，训练开销与GRPO相当，证明其高效实用。"
  ],
  "💡个人观点": "该工作的核心创新在于突破“终局奖励”桎梏，将强化学习监督粒度从“答案对错”下沉至“每步是否忠实”，并通过显隐双路径实现可扩展的步骤级对齐。其截断重采样机制巧妙复用已有前缀，兼顾效率与信号强度，为轻量级推理模型的可信训练提供了新范式。"
}