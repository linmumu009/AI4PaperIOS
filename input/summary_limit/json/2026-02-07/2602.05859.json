{
  "institution": "阿里",
  "short_title": "首套DLM稀疏自编码器框架",
  "📖标题": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
  "🌐来源": "arxiv",
  "paper_id": "2602.05859",
  "🛎️文章简介": {
    "🔸研究问题": "如何为扩散语言模型（DLMs）构建可解释、可干预的机械可解释性工具？",
    "🔸主要贡献": "提出了首个面向DLMs的稀疏自编码器（SAE）框架DLM-Scope，验证其能提取高可解释特征，并发现SAE插入早期层可降低交叉熵损失这一DLM特有现象。"
  },
  "📝重点思路": [
    "🔸设计DLM专属SAE：分别从扩散掩码序列的MASK/UNMASK位置提取残差流激活，适配随机腐蚀机制。",
    "🔸引入时间步steering：在每步去噪中重复注入特征方向，支持ALL-TOKENS与UPDATE-TOKENS两种干预。",
    "🔸构建解码顺序分析范式：基于SAE特征集Jaccard相似度量化S_pre（预解码稳定性）与D_post（后解码漂移），揭示remasking策略的表征动态差异。",
    "🔸验证SAE跨阶段泛化：基座训练SAE在指令微调DLM上即插即用，除最深层外各层功能一致性高。"
  ],
  "🔎分析总结": [
    "🔸SAE插入DLM早期层（L1/L5）在低L0稀疏度下显著降低masked-token交叉熵损失（ΔLM<0），LLM中该现象缺失或极弱。",
    "🔸DLM-SAE在深层（L23/L27）的steering得分达LLM-SAE的2–10倍，表明DLM语义方向更集中于深层残差流。",
    "🔸置信度驱动解码（TOPK-MARGIN/ENTROPY）引发更强早期特征变换与深层后解码漂移，与GSM8K性能（56%–59% vs 8%）正相关。",
    "🔸基座训练SAE在指令微调DLM上对L1–L23层近乎无损迁移，仅L27层明显退化，说明指令调优主要重塑最深层表征。"
  ],
  "💡个人观点": "该工作开创性地将SAE从AR架构迁移至扩散范式，不仅解决方法论适配难题（如位置采样、多步干预），更揭示DLM内在机理:早期层存在冗余信息可被SAE压缩优化，深层则承载强任务导向语义；其提出的解码动态分析法为理解DLM生成过程提供了可量化的新视角，为可控生成与高效微调奠定基础。"
}