{
  "institution": "南洋理工",
  "short_title": "用动态滑动块提升dLLM质量与效率",
  "📖标题": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
  "🌐来源": "arxiv",
  "paper_id": "2602.05992",
  "🛎️文章简介": {
    "🔸研究问题": "如何在不增加训练开销的前提下，优化扩散大语言模型（dLLM）的块调度策略，以同时提升生成质量与推理效率？",
    "🔸主要贡献": "提出无需训练的动态滑动块（DSB）调度方法及配套的DSB Cache机制，在保持因果性的同时自适应语义难度，显著改善dLLM的质量-速度权衡。"
  },
  "📝重点思路": [
    "🔸揭示固定块调度缺陷：刚性分块忽略token置信度差异，致高置信位置延迟解码、低置信位置过早承诺，损害生成质量与并行效率。",
    "🔸提出动态滑动块（DSB）算法：以首个未解码位置为左界、受max_block_size约束动态扩展右界，实现语义驱动的渐进式解码。",
    "🔸设计DSB Cache：引入可变长prefix window，每步联合更新活动块与prefix window的KV缓存，并周期性全局刷新以保障一致性。",
    "🔸采用纯推理时（training-free）设计：全程依赖运行时置信度估计与序列状态，无需微调或新增参数，即插即用于主流dLLM。"
  ],
  "🔎分析总结": [
    "🔸DSB在LLaDA、Dream等模型及GSM8K、HumanEval等基准上一致提升准确率与吞吐量，带缓存时性能逼近甚至超越朴素采样。",
    "🔸前缀窗口是DSB Cache关键：移除后准确率降3–4%，吞吐量降超20%，证实其稳定滑动边界的作用。",
    "🔸DSB对初始块长S_init鲁棒，始终优于固定块；S_max增大可提效但可能降质，需依模型调优。",
    "🔸Dream系列因AR初始化训练，DSB增益弱于LLaDA，表明模型结构影响调度收益，但仍具改进空间。"
  ],
  "💡个人观点": "该工作创新性在于将“语义难度感知”从启发式/训练式方案转向轻量、通用、即插即用的运行时调度机制。DSB不是简单调整块大小，而是重构块演化逻辑——让块成为解码进程的动态投影，兼具理论简洁性与工程实用性。其与缓存机制的协同设计，体现了对dLLM推理栈全链路（调度+缓存）的系统性优化思维。"
}