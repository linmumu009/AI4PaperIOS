{
  "institution": "约翰霍普金斯",
  "short_title": "用共享LoRA子空间实现近严格持续学习",
  "📖标题": "Shared LoRA Subspaces for almost Strict Continual Learning",
  "🌐来源": "arxiv",
  "paper_id": "2602.06043",
  "🛎️文章简介": {
    "🔸研究问题": "如何在不依赖数据回放、不增加模型参数、不使用多个适配器的前提下，实现大模型的参数高效持续学习？",
    "🔸主要贡献": "提出Share方法，通过动态维护一个共享的低秩子空间，以极低参数量（最高100×压缩）和内存开销（281×节省）实现近严格持续学习，并支持跨任务知识正向与反向迁移。"
  },
  "📝重点思路": [
    "🔸堆叠多LoRA（A/B矩阵）并中心化，SVD提取前k个主成分作冻结共享基（α⁰, β⁰），仅训练轻量系数εᵗ。",
    "🔸持续学习分三阶段：初始化子空间、临时扩展φ个基并优化对应系数、融合后重构适配器并重SVD更新基，用伪逆解析求各任务系数。",
    "🔸所有更新为数据/梯度无关的解析操作；临时参数量仅φ(n+d+2p)，远低于标准LoRA的r(n+d)；系数通过Moore-Penrose伪逆投影更新。",
    "🔸支持混合输入流（新数据或新LoRA），可增量集成社区LoRA资源，无需原始数据或任务标签。"
  ],
  "🔎分析总结": [
    "🔸Share-full以0.012M参数（LoRA的1%）在GLUE持续学习中达联合训练性能（83.44% vs 83.90%），并引发显著反向迁移（CoLA +3.81）。",
    "🔸Share以0.10M参数在CIFAR-100上匹配上界精度（94.20%），遗忘率仅0.40%，优于所有prompt/adapter方法。",
    "🔸Share以1M参数（较基线少96%）在3D姿态估计强遮挡场景下超越带回放的iNeMO。",
    "🔸Share单组基向量压缩500个LoRA，在Flux文本生成中实现96×内存节省，OOD任务Rouge-L达55.89。"
  ],
  "💡个人观点": "该工作首次在理论保证下实现参数高效持续学习：不增参、不回放、不存多模型；通过动态构建通用权重子空间及解析式融合，规避梯度干扰，支持知识沉淀与双向迁移。"
}